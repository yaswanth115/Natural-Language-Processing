{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d9bc8d2-087e-461e-ad79-4bf9582f40ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8a7e616-00fb-4197-9479-1b1309a992d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=\"\"\"Hello welcome,to krish naik's NLP tutorials.\n",
    "Please do watch the entire course! to.\n",
    "\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "307ae7a6-9041-486e-be2b-507c4c555910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello welcome,to krish naik's NLP tutorials.\n",
      "Please do watch the entire course! to.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f81c6f09-00cb-4078-bfc9-be45282b9587",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization\n",
    "#para------>sentence \n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7c3e70d-2271-4bd5-b6ca-6d1d588423f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66d59df6-75b6-4e76-807c-718a58d53975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91024e14-6be4-4c91-b6e8-caf3713a77d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello welcome,to krish naik's NLP tutorials.\n",
      "Please do watch the entire course!\n",
      "to.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for sentence in documents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "412b4873-4f62-4cbe-a3b8-a7443f6b6b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#tokenization \n",
    "#para ---> words\n",
    "# sentence ----> words\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ea761ef-b38e-4058-afc4-6f0fff6a38ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'krish',\n",
       " 'naik',\n",
       " \"'s\",\n",
       " 'NLP',\n",
       " 'tutorials',\n",
       " '.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'watch',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'course',\n",
       " '!',\n",
       " 'to',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c867b49-96ab-4fa7-ae17-784be1003d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'welcome', ',', 'to', 'krish', 'naik', \"'s\", 'NLP', 'tutorials', '.']\n",
      "['Please', 'do', 'watch', 'the', 'entire', 'course', '!']\n",
      "['to', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for sentence in documents:\n",
    "    print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce6f6b7d-df66-4da7-bd27-216ba10e0618",
   "metadata": {},
   "outputs": [],
   "source": [
    "#punct library -- it treated a word eg:it's-->\"it\",\"s\"\n",
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba74288e-ad30-45c7-8a1d-98133dc94fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'krish',\n",
       " 'naik',\n",
       " \"'\",\n",
       " 's',\n",
       " 'NLP',\n",
       " 'tutorials',\n",
       " '.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'watch',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'course',\n",
       " '!',\n",
       " 'to',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7723c40-7906-4035-9ee3-1e331e02de41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it will add fullstop as one word but at end of para it shows separate\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c5d3806-9967-486f-9d7c-49b2c8f970e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b41cfc17-15f7-45b8-ae04-95b1ca907518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'krish',\n",
       " 'naik',\n",
       " \"'s\",\n",
       " 'NLP',\n",
       " 'tutorials.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'watch',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'course',\n",
       " '!',\n",
       " 'to',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68acac6e-2763-400a-83a1-349af4108e79",
   "metadata": {},
   "source": [
    "### \n",
    "STEMMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00773e72-4a2b-4562-bae0-acce079f7cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[\"eating\",\"eats\",\"eaten\",\"writing\",\"programming\",\"programs\",\"final\",\"finally\",\"history\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edeec27e-e41a-48e9-a110-7c5a6c6a5763",
   "metadata": {},
   "source": [
    "### Porter stemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c54c17c-fb87-48aa-b0c1-fe6fda78b21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad0cecba-7f12-403b-8848-9e4c70f22232",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stemming=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aca86073-eaeb-4f3a-892b-807b4b7c8036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating----->eat\n",
      "eats----->eat\n",
      "eaten----->eaten\n",
      "writing----->write\n",
      "programming----->program\n",
      "programs----->program\n",
      "final----->final\n",
      "finally----->final\n",
      "history----->histori\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\"----->\"+stemming.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd10b2f1-8ecf-49ac-afa4-f821f9f43bca",
   "metadata": {},
   "source": [
    "### RegxStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d40494eb-0da9-4c50-b9af-a5879583926b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b60309fb-dcd7-42d7-a177-627df6c4c67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_stemmer=RegexpStemmer('ing$|s$|e$|able$',min=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa1ff41f-0cf4-4a77-87d5-065e6d5dfe0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem('eating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e1d8071-0e4e-4f18-a99d-a158e200eab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ingeat'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem('ingeating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce059e75-c67d-43a9-ba2e-f99fa2cd19d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_stemmer=RegexpStemmer('ing|s$|e$|able$',min=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a748295-cf3d-43c1-83d7-877eccb42b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem('ingeating')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a42c03-8323-45b3-9f3c-ba43f9a47e0b",
   "metadata": {},
   "source": [
    "### \n",
    "Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "46ba4d34-3474-4043-b48c-f0859494b772",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3574a71-9ca2-4bf8-8358-4996889858cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "snow=SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d1cad80-b3af-4330-97cc-d71afcca845f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating------>eat\n",
      "eats------>eat\n",
      "eaten------>eaten\n",
      "writing------>write\n",
      "programming------>program\n",
      "programs------>program\n",
      "final------>final\n",
      "finally------>final\n",
      "history------>histori\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\"------>\"+snow.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0f0e6e-9673-4de2-b85b-f12d5248041c",
   "metadata": {},
   "source": [
    "#diff b/w snowball & porter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "771c2b8b-a822-4acc-b478-be25ede090b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fairli'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem(\"fairly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3961c3b5-d131-4c77-85e3-54a0c1f5e320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fair'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snow.stem(\"fairly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca71cdc-215a-4e67-b6db-8bbf2af5fc06",
   "metadata": {},
   "source": [
    "  ### Lemmatization = used in Q/A, Chatbots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "174d4ee0-5c0b-4a50-a6e3-0cc5dde28406",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yaswa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cdbf7031-4c85-49d2-bb53-4829a8654fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b18df454-6d37-4af2-b8b9-72cdae061c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[\"eating\",\"eats\",\"eaten\",\"writing\",\"programming\",\"programs\",\"final\",\"finally\",\"history\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c15bb11c-f564-4608-825a-105009638875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating------>eat\n",
      "eats------>eat\n",
      "eaten------>eat\n",
      "writing------>write\n",
      "programming------>program\n",
      "programs------>program\n",
      "final------>final\n",
      "finally------>finally\n",
      "history------>history\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\"------>\"+lemmatizer.lemmatize(word,pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "518c5a3a-cbee-4568-9470-38383863d247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"goes\",pos='v') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f4b1fc48-2781-418d-9e07-1e82b6542d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fairly'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"fairly\",pos='v')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c324381b-3bdb-4b32-9dc3-59f35b98f3ef",
   "metadata": {},
   "source": [
    "# Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bcbdf355-2609-43fa-9bcd-b6b08a5946ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph=\"\"\"My dear friends,\n",
    "\n",
    "Let us dream, dream, dream. Dream transforms into thoughts and ideas. To make dreams come true, we need to work hard, have perseverance and above all, self-confidence. With confidence, you can achieve anything.\n",
    "\n",
    "I have a vision for India. It is to see India as a developed nation. A nation that is strong, prosperous, and a global leader. A nation where every citizen has access to quality education, healthcare, and opportunities.\n",
    "\n",
    "To achieve this vision, we need to harness the power of our youth. You, the young generation, are the future of India. You have the potential to change the world. Dream big, work hard, and never give up.\n",
    "\n",
    "Let us work together to build a strong, prosperous, and inclusive India. An India that is the pride of every Indian.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "126d967a-a2fa-4759-8742-f3dc337f55a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5e8c9113-a99d-4662-bd8b-1a7165687421",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yaswa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fffb0573-0ce0-4196-89e6-273bfad50627",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d9b18317-29b8-4a6a-b759-0f3a426eabee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e458922a-727d-4df7-af95-a33e7c8ebb32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aber',\n",
       " 'alle',\n",
       " 'allem',\n",
       " 'allen',\n",
       " 'aller',\n",
       " 'alles',\n",
       " 'als',\n",
       " 'also',\n",
       " 'am',\n",
       " 'an',\n",
       " 'ander',\n",
       " 'andere',\n",
       " 'anderem',\n",
       " 'anderen',\n",
       " 'anderer',\n",
       " 'anderes',\n",
       " 'anderm',\n",
       " 'andern',\n",
       " 'anderr',\n",
       " 'anders',\n",
       " 'auch',\n",
       " 'auf',\n",
       " 'aus',\n",
       " 'bei',\n",
       " 'bin',\n",
       " 'bis',\n",
       " 'bist',\n",
       " 'da',\n",
       " 'damit',\n",
       " 'dann',\n",
       " 'der',\n",
       " 'den',\n",
       " 'des',\n",
       " 'dem',\n",
       " 'die',\n",
       " 'das',\n",
       " 'dass',\n",
       " 'daß',\n",
       " 'derselbe',\n",
       " 'derselben',\n",
       " 'denselben',\n",
       " 'desselben',\n",
       " 'demselben',\n",
       " 'dieselbe',\n",
       " 'dieselben',\n",
       " 'dasselbe',\n",
       " 'dazu',\n",
       " 'dein',\n",
       " 'deine',\n",
       " 'deinem',\n",
       " 'deinen',\n",
       " 'deiner',\n",
       " 'deines',\n",
       " 'denn',\n",
       " 'derer',\n",
       " 'dessen',\n",
       " 'dich',\n",
       " 'dir',\n",
       " 'du',\n",
       " 'dies',\n",
       " 'diese',\n",
       " 'diesem',\n",
       " 'diesen',\n",
       " 'dieser',\n",
       " 'dieses',\n",
       " 'doch',\n",
       " 'dort',\n",
       " 'durch',\n",
       " 'ein',\n",
       " 'eine',\n",
       " 'einem',\n",
       " 'einen',\n",
       " 'einer',\n",
       " 'eines',\n",
       " 'einig',\n",
       " 'einige',\n",
       " 'einigem',\n",
       " 'einigen',\n",
       " 'einiger',\n",
       " 'einiges',\n",
       " 'einmal',\n",
       " 'er',\n",
       " 'ihn',\n",
       " 'ihm',\n",
       " 'es',\n",
       " 'etwas',\n",
       " 'euer',\n",
       " 'eure',\n",
       " 'eurem',\n",
       " 'euren',\n",
       " 'eurer',\n",
       " 'eures',\n",
       " 'für',\n",
       " 'gegen',\n",
       " 'gewesen',\n",
       " 'hab',\n",
       " 'habe',\n",
       " 'haben',\n",
       " 'hat',\n",
       " 'hatte',\n",
       " 'hatten',\n",
       " 'hier',\n",
       " 'hin',\n",
       " 'hinter',\n",
       " 'ich',\n",
       " 'mich',\n",
       " 'mir',\n",
       " 'ihr',\n",
       " 'ihre',\n",
       " 'ihrem',\n",
       " 'ihren',\n",
       " 'ihrer',\n",
       " 'ihres',\n",
       " 'euch',\n",
       " 'im',\n",
       " 'in',\n",
       " 'indem',\n",
       " 'ins',\n",
       " 'ist',\n",
       " 'jede',\n",
       " 'jedem',\n",
       " 'jeden',\n",
       " 'jeder',\n",
       " 'jedes',\n",
       " 'jene',\n",
       " 'jenem',\n",
       " 'jenen',\n",
       " 'jener',\n",
       " 'jenes',\n",
       " 'jetzt',\n",
       " 'kann',\n",
       " 'kein',\n",
       " 'keine',\n",
       " 'keinem',\n",
       " 'keinen',\n",
       " 'keiner',\n",
       " 'keines',\n",
       " 'können',\n",
       " 'könnte',\n",
       " 'machen',\n",
       " 'man',\n",
       " 'manche',\n",
       " 'manchem',\n",
       " 'manchen',\n",
       " 'mancher',\n",
       " 'manches',\n",
       " 'mein',\n",
       " 'meine',\n",
       " 'meinem',\n",
       " 'meinen',\n",
       " 'meiner',\n",
       " 'meines',\n",
       " 'mit',\n",
       " 'muss',\n",
       " 'musste',\n",
       " 'nach',\n",
       " 'nicht',\n",
       " 'nichts',\n",
       " 'noch',\n",
       " 'nun',\n",
       " 'nur',\n",
       " 'ob',\n",
       " 'oder',\n",
       " 'ohne',\n",
       " 'sehr',\n",
       " 'sein',\n",
       " 'seine',\n",
       " 'seinem',\n",
       " 'seinen',\n",
       " 'seiner',\n",
       " 'seines',\n",
       " 'selbst',\n",
       " 'sich',\n",
       " 'sie',\n",
       " 'ihnen',\n",
       " 'sind',\n",
       " 'so',\n",
       " 'solche',\n",
       " 'solchem',\n",
       " 'solchen',\n",
       " 'solcher',\n",
       " 'solches',\n",
       " 'soll',\n",
       " 'sollte',\n",
       " 'sondern',\n",
       " 'sonst',\n",
       " 'über',\n",
       " 'um',\n",
       " 'und',\n",
       " 'uns',\n",
       " 'unsere',\n",
       " 'unserem',\n",
       " 'unseren',\n",
       " 'unser',\n",
       " 'unseres',\n",
       " 'unter',\n",
       " 'viel',\n",
       " 'vom',\n",
       " 'von',\n",
       " 'vor',\n",
       " 'während',\n",
       " 'war',\n",
       " 'waren',\n",
       " 'warst',\n",
       " 'was',\n",
       " 'weg',\n",
       " 'weil',\n",
       " 'weiter',\n",
       " 'welche',\n",
       " 'welchem',\n",
       " 'welchen',\n",
       " 'welcher',\n",
       " 'welches',\n",
       " 'wenn',\n",
       " 'werde',\n",
       " 'werden',\n",
       " 'wie',\n",
       " 'wieder',\n",
       " 'will',\n",
       " 'wir',\n",
       " 'wird',\n",
       " 'wirst',\n",
       " 'wo',\n",
       " 'wollen',\n",
       " 'wollte',\n",
       " 'würde',\n",
       " 'würden',\n",
       " 'zu',\n",
       " 'zum',\n",
       " 'zur',\n",
       " 'zwar',\n",
       " 'zwischen']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "13a2abe1-4c43-4b83-9e27-edebfdd24bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "84ca97e7-fecd-47f0-9061-392913f92d8f",
   "metadata": {},
   "source": [
    "stemmer=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "51dca2a6-c557-4318-981d-7d96fd16445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "21e97f30-edf6-4adf-9935-deb4bf25f759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fa4f5137-958a-4e3e-932e-77151d1e6a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Apply stopwords and filter and then apply stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer=PorterStemmer()\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i]=' '.join(words)#coverting all list of words into sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "46c1bd2d-e47c-4980-b753-6e522dad8901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my dear friend , let us dream , dream , dream .',\n",
       " 'dream transform thought idea .',\n",
       " 'to make dream come true , need work hard , persever , self-confid .',\n",
       " 'with confid , achiev anyth .',\n",
       " 'i vision india .',\n",
       " 'it see india develop nation .',\n",
       " 'a nation strong , prosper , global leader .',\n",
       " 'a nation everi citizen access qualiti educ , healthcar , opportun .',\n",
       " 'to achiev vision , need har power youth .',\n",
       " 'you , young gener , futur india .',\n",
       " 'you potenti chang world .',\n",
       " 'dream big , work hard , never give .',\n",
       " 'let us work togeth build strong , prosper , inclus india .',\n",
       " 'an india pride everi indian .']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "271338ae-f32e-44a3-bb22-121ea4c05d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1553d603-8282-45ef-ae08-1f81637b7adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i]=' '.join(words)#coverting all list of words into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e92e48e3-3570-4ed5-ae05-94c152a3e428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dear friend , let u dream , dream , dream .',\n",
       " 'dream transform thought idea .',\n",
       " 'make dream come true , need work hard , persever , self-confid .',\n",
       " 'confid , achiev anyth .',\n",
       " 'vision india .',\n",
       " 'see india develop nation .',\n",
       " 'nation strong , prosper , global leader .',\n",
       " 'nation everi citizen access qualiti educ , healthcar , opportun .',\n",
       " 'achiev vision , need har power youth .',\n",
       " ', young gener , futur india .',\n",
       " 'potenti chang world .',\n",
       " 'dream big , work hard , never give .',\n",
       " 'let u work togeth build strong , prosper , inclus india .',\n",
       " 'india pride everi indian .']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cc1bd5b1-9ff1-4b5e-902f-f0819c88ed5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\yaswa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5594f6b9-7c5c-4324-bd1d-d52ee6f0b2af",
   "metadata": {},
   "source": [
    "# Parts of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f7e637d9-33b6-4d2c-a1ee-5fe3095138be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dear', 'JJ'), ('friend', 'NN'), (',', ','), ('let', 'VB'), ('u', 'JJ'), ('dream', 'NN'), (',', ','), ('dream', 'NN'), (',', ','), ('dream', 'NN'), ('.', '.')]\n",
      "[('dream', 'NN'), ('transform', 'NN'), ('thought', 'VBD'), ('idea', 'NN'), ('.', '.')]\n",
      "[('make', 'VB'), ('dream', 'NN'), ('come', 'VB'), ('true', 'JJ'), (',', ','), ('need', 'JJ'), ('work', 'NN'), ('hard', 'RB'), (',', ','), ('persever', 'RB'), (',', ','), ('self-confid', 'JJ'), ('.', '.')]\n",
      "[('confid', 'NN'), (',', ','), ('achiev', 'JJ'), ('anyth', 'NN'), ('.', '.')]\n",
      "[('vision', 'NN'), ('india', 'NN'), ('.', '.')]\n",
      "[('see', 'VB'), ('india', 'JJ'), ('develop', 'VB'), ('nation', 'NN'), ('.', '.')]\n",
      "[('nation', 'NN'), ('strong', 'JJ'), (',', ','), ('prosper', 'NN'), (',', ','), ('global', 'JJ'), ('leader', 'NN'), ('.', '.')]\n",
      "[('nation', 'NN'), ('everi', 'DT'), ('citizen', 'JJ'), ('access', 'NN'), ('qualiti', 'NN'), ('educ', 'NN'), (',', ','), ('healthcar', 'NN'), (',', ','), ('opportun', 'NN'), ('.', '.')]\n",
      "[('achiev', 'JJ'), ('vision', 'NN'), (',', ','), ('need', 'VBP'), ('har', 'JJ'), ('power', 'NN'), ('youth', 'NN'), ('.', '.')]\n",
      "[(',', ','), ('young', 'JJ'), ('gener', 'NN'), (',', ','), ('futur', 'JJ'), ('india', 'NN'), ('.', '.')]\n",
      "[('potenti', 'NN'), ('chang', 'NN'), ('world', 'NN'), ('.', '.')]\n",
      "[('dream', 'NN'), ('big', 'JJ'), (',', ','), ('work', 'VB'), ('hard', 'RB'), (',', ','), ('never', 'RB'), ('give', 'VBP'), ('.', '.')]\n",
      "[('let', 'VB'), ('u', 'JJ'), ('work', 'NN'), ('togeth', 'RB'), ('build', 'VB'), ('strong', 'JJ'), (',', ','), ('prosper', 'NN'), (',', ','), ('inclus', 'JJ'), ('india', 'NN'), ('.', '.')]\n",
      "[('india', 'JJ'), ('pride', 'NN'), ('everi', 'NN'), ('indian', 'JJ'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    #sentences[i]=' '.join(words)#coverting all list of words into sentences\n",
    "    pos_tag=nltk.pos_tag(words)\n",
    "    print(pos_tag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545f689a-c1cd-4757-8a1f-ff5a9b7069c3",
   "metadata": {},
   "source": [
    "### Named-Entity Recognisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bd7459da-6083-49a4-b422-4a8bbb468e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=\"Apple released the iPhone 14 in September 2022. Elon Musk bought Twitter for $44 billion. The United States is a country in North America. I live in Hyderabad, India. The legendary singer, Bob Dylan, won the Nobel Prize in Literature in 2016. Microsoft Corporation is an American multinational technology corporation. On July 20, 1969, Neil Armstrong became the first person to walk on the Moon. The COVID-19 pandemic, which originated in Wuhan, China, spread rapidly worldwide.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "93f04423-3691-49a1-a9cb-aaed594f637e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "words=nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5e508665-c1ca-4695-8f2b-f2f9af5b12f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_elements = nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0a1fcb3a-e04c-4bbe-bdea-f6e8b8bbde75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\yaswa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ec9d9794-654f-4567-8816-8153233e4cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\yaswa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7397d85-6cc7-44a4-bf53-2d52e33aacc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.ne_chunk(tag_elements).draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14239f0e-d63f-43a7-9339-ce79d66a53ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd6b7a1-dc01-47a8-a75d-9c4d72f72baf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
